<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation" />
  <meta name="keywords" content="Artifacts, Code, Multimodal Large Language Models, Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || []

      function gtag() {
        dataLayer.push(arguments)
      }

      gtag("js", new Date())

      gtag("config", "G-PYVRSFMDRL")
    </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation
            </h1>
      
       <div class="is-size-5 publication-authors">
          <span class="author-block">Tencent Hunyuan Team</span>
        </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Tencent-Hunyuan/ArtifactsBenchmark"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/tencent/ArtifactsBenchmark"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Evaluation Data</span>
                  </a>
                </span>

                <!-- Dataset Link.
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/ArtifactsBenchmark-Instruct"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>ArtifactsBenchmark-Instruct</span>
                  </a>
                </span> -->
                
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./images/first_image.png" alt="Teaser" class="teaser-image center" width="60%" />
        </div>
        <!--
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">ArtifactsBenchmark</span> collects problems from periodic contests on <span
            class="dnerf">LeetCode</span>, <span class="dnerf">AtCoder</span>, and <span class="dnerf">Codeforces</span>
          platforms and uses them for constructing a holistic benchmark for evaluating Code LLMs across variety of
          code-related scenarios continuously over time.
        </h2>
        -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              ArtifactsBench is the first automated multimodal evaluation benchmark for LLM-generated visual artifacts that renders dynamic outputs, assesses fidelity and interactivity using MLLM judges guided by fine-grained checklists, and achieves over 94% human preference correlation across 1,825 diverse tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>
  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contamination</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">ArtifactsBenchmark</span> annotates problems with release dates, and thus allows evaluating
              models on problems released during a specific time period. Thus, for a newer model
              with a training-cutoff date <span class="dnerf">D</span>, we can evaluate it on problems released after
              <span class="dnerf">D</span> to measure its generalization on <i>unseen</i> problems.
            </p>
            <!-- side by side images->
            <div class="columns is-centered">
              <img src="./images/contamination1.png" alt="Code Generation Live Evaluation" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/contamination2.png" alt="Test Output Prediction Live Evaluation" class="teaser-image"
                width="48%" class="center" />
            </div>

            <p>
              The above plots depict the performance of models on code generation and test output prediction scenarios
              on problems released over different months. We find that <span class="dnerf">DeepSeek</span> models
              exhibit a stark drop in performance on LeetCode problems released since September 2023, its release date,
              indicating that the earlier problems might be contaminated. In contrast, for
              <span class="dnerf">GPT</span>
              models, the performance is relatively stable across different months.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Holistic Evaluation and Open vs Closed Models</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">ArtifactsBenchmark</span> evaluates models on a variety of code-related scenarios, such as
              code generation, self-repair, test output prediction, and code execution. We find that while model
              performances
              are correlated across different scenarios, there relative performances and ordering can vary (left
              figure).
              For instance, <span class="dnerf">Claude-3-Opus</span> overtakes <span class="dnerf">GPT-4-turbo</span> in
              the
              test output prediction scenario, but not in the code generation scenario. Similarly,
              <span class="dnerf">Mistral-Large</span>
              performs considerably better on natural language reasoning tasks like test output prediction and code
              execution.
            </p>
            <br />
            <div class="columns is-centered">
              <img src="./images/tasks_radar.png" alt="Holistic Evaluation" class="teaser-image" width="44%"
                height="44%" class="center" />
              <img src="./images/lc_barchart.png" alt="Open vesus Closed Models" class="teaser-image" width="48%"
                class="center" />

              <!-- <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image" width="55%"
                height="55%" class="center" /> ->
            </div>
            <p>
              We compare the performance of open access models with closed api-access models on <span
                class="dnerf">ArtifactsBenchmark</span> and find that generally the closed api-access models outperform the
              open models. Particularly, the only open models that surpass the barrier are fine-tuned variants of large
              (30+B parameter) models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Comparisions</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/codegen_performance.png" alt="Code Generation Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/repair_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <div class="columns is-centered">
              <img src="./images/testgen_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/execution_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <p>
              The above plots depict the performance of models on different scenarios considered in <span
                class="dnerf">ArtifactsBenchmark</span>. We find that <span class="dnerf">GPT-4-turbo</span> and <span
                class="dnerf">Claude-3-Opus</span> models perform best across different scenarios. Among open source
              models, <span class="dnerf">DS-Ins-33B</span> and <span class="dnerf">Phind-34B</span> perform the best.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Submitting Custom Models</h2>
          <div class="content has-text-justified">
            <p>
              To submit models to the leaderboard, you can run the evaluation using the evaluation scripts in
              <a href="https://github.com/ArtifactsBenchmark/ArtifactsBenchmark">GitHub</a>. Once you have the results,
              you can fill out <a href="https://forms.gle/h2abvAHh6UnhWzzd9">this form</a>. You will need to fill out
              model details and provide the generated evaluation file with model generations and pass@1 scores. We will
              review the submission and add the model to the leaderboard accordingly.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>
  </section>
  -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Statistics</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/data_statis.png" alt="Dataset Statistics" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The ArtifactsBench benchmark comprises 1,825 high-quality, challenging queries organized into nine distinct categories: Game Development, SVG Generation, Web Applications, Simulations, Data Science, Management Systems, Multimedia Editing, Quick Tools, and Others. This structure ensures broad coverage of practical application domains.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Dataset Construction Pipeline</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/artifactsBenchmark_datacollect.png" alt="Dataset Construction Pipeline" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We organize ArtifactsBench creation as an eight-stage pipeline: Extraction & Filtering, Manual and LLM-based Rewrite & Polish, Classification and Difficulty Filtering, Small Sample Annotation, CheckList Generation, Model Generation, Manual QA Checking and Quality Control, and Final Data Consolidation. This structured process ensures the generation of diverse, high-quality tasks for robust evaluation of visual code generation.
            </p>
          </div>
        </div>
      </div>  
  </section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Multi-Stage Automated Evaluation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/Evaluation_Validity_verification .png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              To ensure the validity of our evaluation framework, we first rigorously assess the MLLM-as-judge approach by measuring its pairwise scoring agreement with human experts on a carefully curated task subset. After confirming its high reliability (achieving >90% agreement), we then deploy this automated judge for large-scale evaluation across the entire benchmark. The scoring process follows a structured three-stage pipeline: (1) Code Extraction, (2) Dynamic Rendering and Capture, and (3) MLLM-as-Judge Assessment.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Performance on ArtifactsBench</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/main_results_overview.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              Our comprehensive evaluation covers over 30 state-of-the-art Large Language Models (LLMs), including both open-source and proprietary systems. The open-source cohort features 23 models spanning several influential families: the Qwen2.5 series, Qwen3 series, Gemma3 series, along with notable standalone models like Seed-Coder-8B-Instruct and the Deepseek series. For proprietary models, we evaluate 8 leading systems: Gemini-2.5-Pro-Preview-05-06, Claude 3.7 Sonnet (20250219), Claude 4 Sonnet (20250514), GPT-4o-2024-11-20, o3-mini-2025-01-31, Seed-thinking-1.5, and Hunyuan-TurboS-preview.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Scores of differenct difficulties</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/Score_of_diff_difficulty.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We organize the benchmark into three tiers of increasing difficulty. Even the best-performing models struggle to surpass 50 points on the most challenging subset, indicating that our benchmark remains far from saturation. Notably, the models' relative rankings remain consistent across all difficulty levels, and each tier maintains strong discriminative power—demonstrating our benchmark's ability to reliably differentiate model capabilities at every level of challenge.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Ranking correlation between ArtifactsBench and WebDev Arena</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/artifact-arena-rank.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We find that the model rankings from ArtifactsBench exhibit a remarkably high correlation with WebDev Arena, achieving a 94.4% consistency score. This score is calculated using the normalized Footrule metric, which quantifies the agreement between two ranked lists.
            </p>
          </div>
        </div>
      </div>
</section>
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">CodeQwen-1.5-Chat performance on ArtifactsBenchmark for problems of different difficulty levels.</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/8_level_statistics.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              Based on algorithmic complexity, we classify ArtifactsBenchmark into three levels (Easy/Medium/Hard).
In Figure 1, we conduct a statistical analysis of CodeQwen-1.5-Chat's performance on code generation tasks
across various languages. For most languages, the codeLLM can answer the majority of easy questions but struggles with medium and hard ones.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Generation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/9_bench_completion_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including function name, function description, and function call cases),a reference solution, and
              a test cases part. Left Figure: an example of the Lisp language. Middle Figure: a file processing
              programming task in AWK language. During the evaluation, the corresponding file processing result
              by the generated code will be compared with the reference answer. Right Figure: an example of the R language.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Explanation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/10_bench_explain_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part (including a complete function),
              a reference Explanation. Left Figure: an example of the Kotlin language. Middle Figure: an example of
              the Lua language. Right Figure: an example of the HTML language.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Completion</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/11_bench_fim_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including an incomplete function ), a reference complete code solution and test cases. Left Figure: an
              span completion example of the C++ language. Middle Figure: a single-line completion example of
              the Rust language. Right Figure: a multiple-line completion example of the Shell language.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Optimization Details</h2>
        <div class="content has-text-justified">
          <div class="columns is-centered">
            <img src="./images/12_ts.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
              height="80%" />
          </div>
          <p>
            All mCoder models are fine-tuned using 8 NVIDIA A800-80GB GPUs. The models are trained for
            2 epochs with a cosine scheduler, starting at a learning rate of 2e-5 and incorporating a 3% warmup
            phase. Training a model takes about 5 hours. We used AdamW (Loshchilov and Hutter, 2017) as
            the optimizer and a batch size of 512 with a sequence truncation length of 4096. We use PyTorch’s
            Fully Sharded Data Parallel (FSDP) to perform distributed training of the model, and use gradient
            checkpointing technology and gradient accumulation to save memory and achieve training with a
            larger batch size.
          </p>
        </div>
      </div>
    </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Analysis of Language Representations</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/12_cluster_merge.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              As shown in the Figure, we analyzed the programming languages in the ArtifactsBenchmark from their presentation perspective.
              We used CodeBERT to extract code representations from code snippets in ArtifactsBenchmark.
              The figure clearly shows that languages with similar syntax have closely related representations.
              For example, other functional programming languages similar to CommonLisp, as well as C, C++, Java, and scripting
              languages, exhibit high grammar similarity.
            </p>
          </div>
        </div>
      </div>
</section> -->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="adamwzhang@tencent.com">adamwzhang@tencent.com</a> for questions or
              feedback on ArtifactsBench. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark. Finally, ArtifactsBench provides one axis of LLM coding evaluations and we recommend the
              following leaderboards for measuring code LM ability on various coding tasks, such as
              <a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench Leaderboard</a>,
              <a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>,
              <a href="https://crux-eval.github.io/leaderboard.html">CruxEval Leaderboard</a>,
              <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a>,
              <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>,
              <a href="https://infi-coder.github.io/inficoder-eval/">InfiCoder-Eval</a>, and
              <a href="https://leaderboard.tabbyml.com/">TabbyML Leaderboard</a>.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/LiveCodeBench/livecodebench.github.io">LiveCodeBench</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
